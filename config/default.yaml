name: "emil_transformer"
train:
  batch_size: 32
  total_token_proc: 40_000_000
  cos_an_lr_max: 3e-4
  cos_an_lr_min: 1e-6
  grad_clip_max_norm: 0.01
  eval_steps: 20
model:
  context_length: 256
  num_layers: 4
  d_model: 512
  num_heads: 16
  d_ff: 1344
  rope_theta: 10_000
optimizer:
  learning_rate: 5e-4
  betas: 
    - 0.9
    - 0.999
  eps: 1e-08
  weight_decay: 0.0